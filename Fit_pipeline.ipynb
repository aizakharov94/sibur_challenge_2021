{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import dill\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from category_encoders import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.base import BaseEstimator\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = \"sc2021_train_deals.csv\"\n",
    "\n",
    "prev_cols_len = 360\n",
    "predictions_len = 30\n",
    "\n",
    "key_cols = [\"material_code\", \"company_code\", \"country\", \"region\", \"manager_code\"]\n",
    "cat_cols = ['material_lvl1_name', 'material_lvl2_name', 'material_lvl3_name']\n",
    "\n",
    "prev_cols = [\"prev_\" + str(i) for i in range(1, prev_cols_len + 1)][::-1]\n",
    "prev_cols_days = [\"prev_\" + str(i) + '_day' for i in range(1, 31)][::-1]\n",
    "future_cols = [\"future_\" + str(i) for i in range(1, predictions_len + 1)]\n",
    "\n",
    "# Даты праздников\n",
    "holidays_list = ['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04', '2018-01-05', '2018-01-08', '2018-02-23',\n",
    "                 '2018-03-08', '2018-03-09', '2018-04-30', '2018-05-01', '2018-05-02', '2018-05-09', '2018-06-11',\n",
    "                 '2018-06-12', '2018-11-05', '2018-12-31', '2019-01-01', '2019-01-02', '2019-01-03', '2019-01-04',\n",
    "                 '2019-01-07', '2019-01-08', '2019-03-08', '2019-05-01', '2019-05-02', '2019-05-03', '2019-05-09',\n",
    "                 '2019-05-10', '2019-06-12', '2019-11-04', '2019-12-31', '2020-01-01', '2020-01-02', '2020-01-03',\n",
    "                 '2020-01-06', '2020-01-07', '2020-01-08', '2020-02-24', '2020-03-09', '2020-05-01', '2020-05-04',\n",
    "                 '2020-05-05', '2020-05-11', '2020-06-12', '2020-11-04', '2020-12-31', '2021-01-01', '2021-01-04',\n",
    "                 '2021-01-05', '2021-01-06', '2021-01-07', '2021-01-08', '2021-02-22', '2021-02-23', '2021-03-08',\n",
    "                 '2021-05-03', '2021-05-04', '2021-05-05', '2021-05-10', '2021-06-14', '2021-11-04', '2021-11-05',\n",
    "                 '2021-12-31', '2022-01-03', '2022-01-04', '2022-01-05', '2022-01-06', '2022-01-07', '2022-02-23',\n",
    "                 '2022-03-07', '2022-03-08', '2022-05-02', '2022-05-03', '2022-05-09', '2022-05-10', '2022-06-13',\n",
    "                 '2022-11-04']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение выборки на каждую дату и заполнение пропусков\n",
    "def get_main_df(table):\n",
    "    \n",
    "    # Download table\n",
    "    df = table.copy()\n",
    "    df = df.groupby(list(df.columns)[:-1])['volume'].sum().reset_index()\n",
    "    df.rename({'volume': 'target'}, axis=1, inplace=True)\n",
    "\n",
    "    # All unique points\n",
    "    unique_rows = df[key_cols].drop_duplicates()\n",
    "    unique_rows['key'] = 0\n",
    "\n",
    "    # Dates range\n",
    "    dates_data = pd.DataFrame({'date': pd.date_range(df['date'].min(), df['date'].max())})\n",
    "    dates_data['key'] = 0\n",
    "    all_rows_date = unique_rows.merge(dates_data, how='outer', on=['key']).drop(['key'], axis=1)\n",
    "\n",
    "    # Min dates\n",
    "    min_dates = df.groupby(key_cols)['date'].min().reset_index()\n",
    "    min_dates.rename({'date': 'min_date'}, axis=1, inplace=True)\n",
    "    all_rows_date = all_rows_date.merge(min_dates, how='left', on=key_cols)\n",
    "    all_rows_date = all_rows_date[all_rows_date['date'] >= all_rows_date['min_date']].reset_index(drop=True)\n",
    "    all_rows_date.drop(['min_date'], axis=1, inplace=True)\n",
    "\n",
    "    # Join main DataFrame\n",
    "    df = all_rows_date.merge(df, how='left', on=key_cols + ['date'])\n",
    "    df['target'] = df['target'].fillna(0).astype(int)\n",
    "    \n",
    "    df = df.sort_values(key_cols + ['date']).reset_index(drop=True)\n",
    "    df.material_lvl1_name = df.material_lvl1_name.fillna(method='ffill')\n",
    "    df.material_lvl2_name = df.material_lvl2_name.fillna(method='ffill')\n",
    "    df.material_lvl3_name = df.material_lvl3_name.fillna(method='ffill')\n",
    "    df.contract_type = df.contract_type.fillna(method='ffill')\n",
    "    df.month = pd.to_datetime(df['date'].astype(str).str[:-2] + '01')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Получение начальной (без признаков) обучающей выборки\n",
    "def get_train_data(df):\n",
    "    df = df.sort_values(key_cols + ['date']).reset_index(drop=True)\n",
    "    \n",
    "    for i in range(1, prev_cols_len + 1):\n",
    "        df['prev_' + str(i)] = df.groupby(key_cols)[\"target\"].shift(i).rolling(30).sum()\n",
    "        \n",
    "    for i in range(1, 31):\n",
    "        df['prev_' + str(i) + '_day'] = df.groupby(key_cols)[\"target\"].shift(i).rolling(1).sum()\n",
    "\n",
    "    df[\"future_1\"] = df['target'].copy()\n",
    "    for i in range(2, predictions_len + 1):\n",
    "        df[\"future_\" + str(i)] = df.groupby(key_cols)['target'].shift(-i+1).rolling(1).mean()\n",
    "\n",
    "    df = df[pd.isnull(df[[\"prev_\" + str(i) for i in range(1, prev_cols_len + 1)][::-1]]).sum(1)\\\n",
    "                                                                    < prev_cols_len].reset_index(drop=True)\n",
    "    df = df[pd.isnull(df[future_cols]).sum(1) == 0].reset_index(drop=True)\n",
    "    df['target'] = df[future_cols].sum(1)\n",
    "    df.drop(future_cols, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Получение признаков на основе дат праздников\n",
    "def get_dayoff_features():    \n",
    "    holidays_table = pd.DataFrame({'date': holidays_list})\n",
    "    holidays_table['year'] = holidays_table['date'].str[:4].astype(int)\n",
    "    holidays_table = holidays_table[['year', 'date']]\n",
    "    dates = holidays_table['date'].astype(str)    \n",
    "    data_hol = pd.DataFrame(pd.date_range('2018-01-01', '2023-01-01'), columns=['merge_date'])\n",
    "    data_hol['merge_date'] = data_hol['merge_date'].astype(str)\n",
    "    data_hol = data_hol.merge(pd.DataFrame({'merge_date': dates, 'is_holiday': 1}), how='left', on=['merge_date'])\n",
    "    data_hol['is_holiday'] = data_hol['is_holiday'].fillna(0).astype(int)\n",
    "    data_hol['weekday'] = pd.to_datetime(data_hol['merge_date']).dt.dayofweek.astype(int)\n",
    "    data_hol['is_weekend'] = data_hol['weekday'].isin({5,6}).astype(int)\n",
    "    data_hol['is_dayoff'] = (data_hol['is_holiday'] | data_hol['is_weekend']).astype(int)\n",
    "    data_hol['merge_date'] = pd.to_datetime(data_hol['merge_date'])\n",
    "    \n",
    "    data_hol['key'] = 0\n",
    "    data_hol = data_hol[['merge_date', 'key']].merge(\n",
    "                data_hol.rename({'merge_date': 'dt'}, axis=1), how='outer', on='key')\n",
    "    data_hol['merge_date_end'] = data_hol['merge_date'] + pd.DateOffset(months=1)\n",
    "    data_hol = data_hol[(data_hol.merge_date <= data_hol['dt']) &\\\n",
    "                        (data_hol.merge_date_end > data_hol['dt'])].reset_index(drop=True)\n",
    "    data_hol = data_hol.groupby(['merge_date']).agg({'is_holiday': ['sum', 'mean'],\n",
    "                                                     'is_weekend': ['sum', 'mean'],\n",
    "                                                     'is_dayoff': ['sum', 'mean']})\n",
    "    data_hol.columns = [\"_\".join(x) for x in data_hol.columns.ravel()]\n",
    "    data_hol = data_hol.reset_index().rename({'merge_date': 'date'}, axis=1)\n",
    "    return data_hol\n",
    "\n",
    "# Получение признаков из категорий OneHot\n",
    "def get_cat_features_fit_transform(df):\n",
    "    \n",
    "    df['is_contract'] = df['contract_type'].isin(['Контракт', 'Contract + Spot']) * 1\n",
    "    df['is_spot'] = df['contract_type'].isin(['Спот', 'Contract + Spot']) * 1\n",
    "    df.drop('contract_type', axis=1, inplace=True)\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        df[col] = df[col].str.replace(' ', '_')\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        df['for_cat_' + col] = df[col].copy()\n",
    "    ohe = OneHotEncoder(cols=['for_cat_' + x for x in cat_cols], use_cat_names=True)\n",
    "    df = ohe.fit_transform(df)\n",
    "    \n",
    "    return df, ohe\n",
    "\n",
    "# Различные статистики из исторических данных\n",
    "def get_stat_features(dfs):\n",
    "    \n",
    "    ### Statistics from previous series values\n",
    "    # 1 month\n",
    "    dfs['1_month_median'] = dfs[prev_cols[-30:]].median(1)\n",
    "    dfs['1_month_mean'] = dfs[prev_cols[-30:]].mean(1)\n",
    "    dfs['1_month_std'] = dfs[prev_cols[-30:]].std(1)\n",
    "    dfs['1_month_max'] = dfs[prev_cols[-30:]].max(1)\n",
    "    dfs['1_month_min'] = dfs[prev_cols[-30:]].min(1)\n",
    "    dfs['1_month_zeros_prop'] = (dfs[prev_cols[-30:]] == 0).mean(1)\n",
    "\n",
    "    # 2 month\n",
    "    dfs['2_month_median'] = dfs[prev_cols[-60:-30]].median(1)\n",
    "    dfs['2_month_mean'] = dfs[prev_cols[-60:-30]].mean(1)\n",
    "    dfs['2_month_std'] = dfs[prev_cols[-60:-30]].std(1)\n",
    "    dfs['2_month_max'] = dfs[prev_cols[-60:-30]].max(1)\n",
    "    dfs['2_month_min'] = dfs[prev_cols[-60:-30]].min(1)\n",
    "    dfs['2_month_zeros_prop'] = (dfs[prev_cols[-60:-30]] == 0).mean(1)\n",
    "\n",
    "    # 1_2 month changes\n",
    "    dfs['month_median_change'] = (dfs['1_month_median'] - dfs['2_month_median']) / (dfs['2_month_median'] + 1e-6)\n",
    "    dfs['month_mean_change'] = (dfs['1_month_mean'] - dfs['2_month_mean']) / (dfs['2_month_mean'] + 1e-6)\n",
    "    dfs['month_std_change'] = (dfs['1_month_std'] - dfs['2_month_std']) / (dfs['2_month_std'] + 1e-6)\n",
    "    dfs['month_max_change'] = (dfs['1_month_max'] - dfs['2_month_max']) / (dfs['2_month_max'] + 1e-6)\n",
    "    dfs['month_min_change'] = (dfs['2_month_min'] - dfs['2_month_min']) / (dfs['2_month_min'] + 1e-6)\n",
    "    dfs['month_zeros_prop_change'] = (dfs['1_month_zeros_prop'] - dfs['2_month_zeros_prop']) /\\\n",
    "                                                                        (dfs['2_month_zeros_prop'] + 1e-6)\n",
    "    # 1-3 months\n",
    "    dfs['1_3_month_median'] = dfs[prev_cols[-90:]].median(1)\n",
    "    dfs['1_3_month_mean'] = dfs[prev_cols[-90:]].mean(1)\n",
    "    dfs['1_3_month_std'] = dfs[prev_cols[-90:]].std(1)\n",
    "    dfs['1_3_month_max'] = dfs[prev_cols[-90:]].max(1)\n",
    "    dfs['1_3_month_min'] = dfs[prev_cols[-90:]].min(1)\n",
    "    dfs['1_3_month_zeros_prop'] = (dfs[prev_cols[-90:]] == 0).mean(1)\n",
    "    \n",
    "    # 1-3 changes to 1 month\n",
    "    dfs['glob_month_median_change'] = (dfs['1_month_median'] - dfs['1_3_month_median']) /\\\n",
    "                                                                (dfs['1_3_month_median'] + 1e-6)\n",
    "    dfs['glob_month_mean_change'] = (dfs['1_month_mean'] - dfs['1_3_month_mean']) /\\\n",
    "                                                                (dfs['1_3_month_mean'] + 1e-6)\n",
    "    dfs['glob_month_max_change'] = (dfs['1_month_max'] - dfs['1_3_month_max']) / (dfs['1_3_month_max'] + 1e-6)\n",
    "    dfs['glob_month_min_change'] = (dfs['1_month_min'] - dfs['1_3_month_min']) / (dfs['1_3_month_min'] + 1e-6)\n",
    "    dfs['glob_month_zeros_prop_change'] = (dfs['1_month_zeros_prop'] - dfs['1_3_month_zeros_prop']) /\\\n",
    "                                                                        (dfs['1_3_month_zeros_prop'] + 1e-6)\n",
    "    \n",
    "    # year\n",
    "    dfs['year_median'] = dfs[prev_cols[-360:]].median(1)\n",
    "    dfs['year_mean'] = dfs[prev_cols[-360:]].mean(1)\n",
    "    dfs['year_std'] = dfs[prev_cols[-360:]].std(1)\n",
    "    dfs['year_max'] = dfs[prev_cols[-360:]].max(1)\n",
    "    dfs['year_min'] = dfs[prev_cols[-360:]].min(1)\n",
    "    dfs['year_zeros_prop'] = (dfs[prev_cols[-360:]] == 0).mean(1)\n",
    "    \n",
    "    # half year 1\n",
    "    dfs['1_half_year_median'] = dfs[prev_cols[-180:]].median(1)\n",
    "    dfs['1_half_year_mean'] = dfs[prev_cols[-180:]].mean(1)\n",
    "    dfs['1_half_year_std'] = dfs[prev_cols[-180:]].std(1)\n",
    "    dfs['1_half_year_max'] = dfs[prev_cols[-180:]].max(1)\n",
    "    dfs['1_half_year_min'] = dfs[prev_cols[-180:]].min(1)\n",
    "    dfs['1_half_year_zeros_prop'] = (dfs[prev_cols[-180:]] == 0).mean(1)\n",
    "    \n",
    "    # half year 2\n",
    "    dfs['2_half_year_median'] = dfs[prev_cols[-360:-180]].median(1)\n",
    "    dfs['2_half_year_mean'] = dfs[prev_cols[-360:-180]].mean(1)\n",
    "    dfs['2_half_year_std'] = dfs[prev_cols[-360:-180]].std(1)\n",
    "    dfs['2_half_year_max'] = dfs[prev_cols[-360:-180]].max(1)\n",
    "    dfs['2_half_year_min'] = dfs[prev_cols[-360:-180]].min(1)\n",
    "    dfs['2_half_year_zeros_prop'] = (dfs[prev_cols[-360:-180]] == 0).mean(1)\n",
    "    \n",
    "    # half year 1 2 stats\n",
    "    dfs['half_year_median_change'] = (dfs['1_half_year_median'] - dfs['2_half_year_median']) /\\\n",
    "                                                                            (dfs['2_half_year_median'] + 1e-6)\n",
    "    dfs['half_year_week_mean_change'] = (dfs['1_half_year_mean'] - dfs['2_half_year_mean']) /\\\n",
    "                                                                            (dfs['2_half_year_mean'] + 1e-6)\n",
    "    dfs['half_year_week_std_change'] = (dfs['1_half_year_std'] - dfs['2_half_year_std']) /\\\n",
    "                                                                            (dfs['2_half_year_std'] + 1e-6)\n",
    "    dfs['half_year_week_max_change'] = (dfs['1_half_year_max'] - dfs['2_half_year_max']) /\\\n",
    "                                                                            (dfs['2_half_year_max'] + 1e-6)\n",
    "    dfs['half_year_week_min_change'] = (dfs['1_half_year_zeros_prop'] - dfs['2_half_year_min']) /\\\n",
    "                                                                            (dfs['2_half_year_min'] + 1e-6)\n",
    "    dfs['half_year_week_zeros_prop_change'] = (dfs['1_half_year_zeros_prop'] - dfs['2_half_year_zeros_prop']) /\\\n",
    "                                                                            (dfs['2_half_year_zeros_prop'] + 1e-6)\n",
    "    \n",
    "    # year changes to 1 month\n",
    "    dfs['glob_year_median_change'] = (dfs['1_month_median'] - dfs['year_median']) / (dfs['year_median'] + 1e-6)\n",
    "    dfs['glob_year_mean_change'] = (dfs['1_month_mean'] - dfs['year_mean']) / (dfs['1_3_month_mean'] + 1e-6)\n",
    "    dfs['glob_year_max_change'] = (dfs['1_month_max'] - dfs['year_max']) / (dfs['year_max'] + 1e-6)\n",
    "    dfs['glob_year_min_change'] = (dfs['1_month_min'] - dfs['year_min']) / (dfs['year_min'] + 1e-6)\n",
    "    dfs['glob_year_zeros_prop_change'] = (dfs['1_month_zeros_prop'] - dfs['year_zeros_prop']) /\\\n",
    "                                                                        (dfs['year_zeros_prop'] + 1e-6) \n",
    "    # half year changes to 1 month\n",
    "    dfs['glob_half_year_median_change'] = (dfs['1_month_median'] - dfs['1_half_year_median']) /\\\n",
    "                                                                    (dfs['1_half_year_median'] + 1e-6)\n",
    "    dfs['glob_half_year_mean_change'] = (dfs['1_month_mean'] - dfs['1_half_year_mean']) /\\\n",
    "                                                                    (dfs['1_half_year_mean'] + 1e-6)\n",
    "    dfs['glob_half_year_max_change'] = (dfs['1_month_max'] - dfs['1_half_year_max']) /\\\n",
    "                                                                    (dfs['1_half_year_max'] + 1e-6)\n",
    "    dfs['glob_half_year_min_change'] = (dfs['1_month_min'] - dfs['1_half_year_min']) /\\\n",
    "                                                                    (dfs['1_half_year_min'] + 1e-6)\n",
    "    dfs['glob_half_year_zeros_prop_change'] = (dfs['1_month_zeros_prop'] - dfs['1_half_year_zeros_prop']) /\\\n",
    "                                                                        (dfs['1_half_year_zeros_prop'] + 1e-6) \n",
    "    \n",
    "    # 1 week\n",
    "    dfs['1_week_median'] = dfs[prev_cols[-7:]].median(1)\n",
    "    dfs['1_week_mean'] = dfs[prev_cols[-7:]].mean(1)\n",
    "    dfs['1_week_std'] = dfs[prev_cols[-7:]].std(1)\n",
    "    dfs['1_week_max'] = dfs[prev_cols[-7:]].max(1)\n",
    "    dfs['1_week_min'] = dfs[prev_cols[-7:]].min(1)\n",
    "    dfs['1_week_zeros_prop'] = (dfs[prev_cols[-7:]] == 0).mean(1)\n",
    "\n",
    "    # 2 week\n",
    "    dfs['2_week_median'] = dfs[prev_cols[-14:-7]].median(1)\n",
    "    dfs['2_week_mean'] = dfs[prev_cols[-14:-7]].mean(1)\n",
    "    dfs['2_week_std'] = dfs[prev_cols[-14:-7]].std(1)\n",
    "    dfs['2_week_max'] = dfs[prev_cols[-14:-7]].max(1)\n",
    "    dfs['2_week_min'] = dfs[prev_cols[-14:-7]].min(1)\n",
    "    dfs['2_week_zeros_prop'] = (dfs[prev_cols[-14:-7]] == 0).mean(1)\n",
    "    \n",
    "    # 1_2 week changes\n",
    "    dfs['week_median_change'] = (dfs['1_week_median'] - dfs['2_week_median']) / (dfs['2_week_median'] + 1e-6)\n",
    "    dfs['week_mean_change'] = (dfs['1_week_mean'] - dfs['2_week_mean']) / (dfs['2_week_mean'] + 1e-6)\n",
    "    dfs['week_std_change'] = (dfs['1_week_std'] - dfs['2_week_std']) / (dfs['2_week_std'] + 1e-6)\n",
    "    dfs['week_max_change'] = (dfs['1_week_max'] - dfs['2_week_max']) / (dfs['2_week_max'] + 1e-6)\n",
    "    dfs['week_min_change'] = (dfs['2_week_min'] - dfs['2_week_min']) / (dfs['2_week_min'] + 1e-6)\n",
    "    dfs['week_zeros_prop_change'] = (dfs['1_week_zeros_prop'] - dfs['2_week_zeros_prop']) /\\\n",
    "                                                                        (dfs['2_week_zeros_prop'] + 1e-6)\n",
    "    # 1 day statistics\n",
    "    dfs['1_2_day_perc'] = (dfs['prev_1'] - dfs['prev_2']) / (dfs['prev_2'] + 1e-6)\n",
    "    dfs['1_2_day_diff'] = dfs['prev_1'] - dfs['prev_2']\n",
    "    dfs['1_day_month_change_mean'] = (dfs['prev_1'] - dfs['1_month_mean']) / (dfs['1_month_mean'] + 1e-6)\n",
    "    dfs['1_day_month_change_median'] = (dfs['prev_1'] - dfs['1_month_median']) / (dfs['1_month_median'] + 1e-6)\n",
    "    dfs['1_day_month_change_max'] = (dfs['prev_1'] - dfs['1_month_max']) / (dfs['1_month_max'] + 1e-6)\n",
    "    dfs['1_day_month_change_min'] = (dfs['prev_1'] - dfs['1_month_min']) / (dfs['1_month_min'] + 1e-6)\n",
    "    dfs['1_day_week_change_mean'] = (dfs['prev_1'] - dfs['1_week_mean']) / (dfs['1_week_mean'] + 1e-6)\n",
    "    dfs['1_day_week_change_median'] = (dfs['prev_1'] - dfs['1_week_median']) / (dfs['1_week_median'] + 1e-6)\n",
    "    dfs['1_day_week_change_max'] = (dfs['prev_1'] - dfs['1_week_max']) / (dfs['1_week_max'] + 1e-6)\n",
    "    dfs['1_day_week_change_min'] = (dfs['prev_1'] - dfs['1_week_min']) / (dfs['1_week_min'] + 1e-6)  \n",
    "    \n",
    "    return dfs\n",
    "\n",
    "# Получение последнего ненулевого значения и сколько дней прошло с него\n",
    "def get_last_action(df):\n",
    "    dense = ((df[prev_cols] != 0) & (~df[prev_cols].isnull())).multiply(range(prev_cols_len, 0, -1), axis=1)\n",
    "    dense[dense == 0] = 999\n",
    "    df['days_from_last_non_zero_action'] = dense.idxmin(axis=1).str.split('_').str[-1].astype(int)\n",
    "    df['last_non_zero_value'] = np.array(df[prev_cols])[range(df.shape[0]),\n",
    "                                                prev_cols_len - df['days_from_last_non_zero_action'].values]\n",
    "    dense = ((df[prev_cols] == 0) & (~df[prev_cols].isnull())).multiply(range(prev_cols_len, 0, -1), axis=1)\n",
    "    dense[dense == 0] = 999\n",
    "    df['days_from_last_zero_action'] = dense.idxmin(axis=1).str.split('_').str[-1].astype(int)\n",
    "    return df\n",
    "\n",
    "# Признак: сколько дней были одинаковые значения\n",
    "def add_lr_features(df):    \n",
    "    buffs = pd.DataFrame((df[prev_cols[1:]].values != df[prev_cols[:-1]].values) |\\\n",
    "                         (df[prev_cols[1:]].values == 0)).multiply(range(prev_cols_len - 1, 0, -1), axis=1)\n",
    "    buffs = (buffs != 0).multiply(range(prev_cols_len - 1), axis=1)\n",
    "    df['equal_days_befor_target'] = prev_cols_len - 1 - buffs.idxmax(1)    \n",
    "    return df\n",
    "\n",
    "# Признаки на основе длин массивов нулей за историю\n",
    "def get_zeros_mass_features(df):\n",
    "    def get_zeros_length(x):\n",
    "        return np.arange(len(x))[(x == 0) & (np.concatenate([x[1:], np.array([-1])]) != 0)] -\\\n",
    "               np.arange(len(x))[(x == 0) & (np.concatenate([np.array([-1]), x[:-1]]) != 0)] + 1\n",
    "    df['zeros_masses'] = df[prev_cols].apply(lambda x: get_zeros_length(x.values), axis=1)\n",
    "    df['last_zeros_len'] = df['zeros_masses'].apply(lambda x: x[-1] if len(x) > 0 else -1)\n",
    "    df['zeros_mass_mean'] = df['zeros_masses'].apply(lambda x: np.mean(x) if len(x) > 0 else -1)\n",
    "    df['zeros_mass_median'] = df['zeros_masses'].apply(lambda x: np.median(x) if len(x) > 0 else -1)\n",
    "    df['zeros_mass_std'] = df['zeros_masses'].apply(lambda x: np.std(x) if len(x) > 0 else -1)\n",
    "    df['zeros_mass_cnt'] = df['zeros_masses'].apply(lambda x: len(x))\n",
    "    df['zeros_mass_max'] = df['zeros_masses'].apply(lambda x: np.max(x) if len(x) > 0 else -1)\n",
    "    df.drop(['zeros_masses'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Признаки на основе истории по дням за последний месяц\n",
    "def get_last_days_features(df):\n",
    "    df['last_day_total'] = df['prev_1_day'] * 30\n",
    "    df['last_week_total'] = df[prev_cols_days[-7:]].sum(1) / 7 * 30\n",
    "    df['last_2_week_total'] = df[prev_cols_days[-14:-7]].sum(1) / 7 * 30\n",
    "    df['last_month_total'] = df['prev_1'].copy()\n",
    "    \n",
    "    df['perc_last_day_week_total'] = df['last_day_total'] / (df['last_week_total'] + 1e-6)\n",
    "    df['perc_last_day_month_total'] = df['last_day_total'] / (df['last_month_total'] + 1e-6)\n",
    "    df['perc_last_week_month_total'] = df['last_week_total'] / (df['last_month_total'] + 1e-6)\n",
    "    df['perc_last_double_weeks_month_total'] = (df['last_week_total'] + df['last_week_total']) /\\\n",
    "                                                                        (2 * df['last_month_total'] + 1e-6)\n",
    "    df['perc_last_week_2_week_total'] = df['last_week_total'] / (df['last_2_week_total'] + 1e-6)\n",
    "    df['perc_last_week_double_week_total'] = df['last_week_total'] * 2 /\\\n",
    "                                                (df['last_2_week_total'] + df['last_week_total'] + 1e-6)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 11s, sys: 1min 37s, total: 5min 48s\n",
      "Wall time: 5min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv(pathlib.Path(\".\").joinpath(DATA_FILE), parse_dates=[\"month\", \"date\"])\n",
    "df = get_main_df(df)\n",
    "df = get_train_data(df)\n",
    "a = df.columns\n",
    "df, ohe = get_cat_features_fit_transform(df)\n",
    "pickle.dump(ohe, open('./ohe.pkl', 'wb'))\n",
    "df = df.merge(get_dayoff_features(), how='left', on=['date'])\n",
    "df = get_stat_features(df)\n",
    "df = get_last_action(df)\n",
    "df = add_lr_features(df)\n",
    "df = get_zeros_mass_features(df)\n",
    "df = get_last_days_features(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {'metric' : 'custom',\n",
    "               'objective': 'regression',\n",
    "               'learning_rate': 0.05,\n",
    "               'boosting_type' : 'gbdt',\n",
    "               'n_jobs' : 10,\n",
    "               'verbose' : -1,\n",
    "\n",
    "               'num_leaves': 30,\n",
    "               'min_child_samples': 28,\n",
    "               'max_depth': 12,\n",
    "               'min_split_gain': 0.0,\n",
    "               'reg_alpha': 0.1,\n",
    "               'reg_lambda': 0.1,\n",
    "               'feature_fraction': 0.8,\n",
    "               'bagging_freq': 2,\n",
    "               'bagging_fraction': 0.85,\n",
    "               'verbose': -1,\n",
    "}\n",
    "\n",
    "lgbm_params_class = {'metric' : 'auc',\n",
    "               'objective': 'binary',\n",
    "               'learning_rate': 0.05,\n",
    "               'boosting_type' : 'gbdt',\n",
    "               'n_jobs' : 10,\n",
    "               'verbose' : -1,\n",
    "\n",
    "               'num_leaves': 30,\n",
    "               'min_child_samples': 28,\n",
    "               'max_depth': 12,\n",
    "               'min_split_gain': 0.0,\n",
    "               'reg_alpha': 0.1,\n",
    "               'reg_lambda': 0.1,\n",
    "               'feature_fraction': 0.8,\n",
    "               'bagging_freq': 2,\n",
    "               'bagging_fraction': 0.85,\n",
    "               'verbose': -1,\n",
    "}\n",
    "\n",
    "def rmsle_lgbm(y_pred, data):\n",
    "    y_true = np.array(data.get_label())\n",
    "    score = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return 'rmsle', score, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [15:22<00:00,  9.22s/it]\n"
     ]
    }
   ],
   "source": [
    "class MultiLGBMModel(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.dels = ['zeros_masses', 'last_zeros_len', 'zeros_mass_mean', 'zeros_mass_median',\n",
    "                     'zeros_mass_std', 'zeros_mass_cnt', 'zeros_mass_max']\n",
    "        self.models = []\n",
    "        self.lgbm_params = lgbm_params\n",
    "    \n",
    "    def fit(self, df):\n",
    "        self.train_cols = df.columns.difference(key_cols + cat_cols + ['date', 'month', 'target'] + self.dels)\n",
    "        train_data = lgb.Dataset(df[self.train_cols], label=np.log1p(df['target']))\n",
    "        for i in tqdm(range(100)):\n",
    "            lgbm_params['seed'] = i\n",
    "            lgbm = lgb.train(params=self.lgbm_params, train_set=train_data, valid_sets=[train_data],\n",
    "                             num_boost_round=100, verbose_eval=1000, feval=rmsle_lgbm)\n",
    "            self.models.append(lgbm)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, test):\n",
    "        preds_df = pd.DataFrame()\n",
    "        for i in range(100):\n",
    "            preds_df['pred_' + str(i)] = self.models[i].predict(test[self.train_cols])\n",
    "        return preds_df.mean(1)\n",
    "\n",
    "model = MultiLGBMModel()\n",
    "model.fit(df)\n",
    "dill.dump(model, file = open(\"./model.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dels = ['zeros_masses', 'last_zeros_len', 'zeros_mass_mean', 'zeros_mass_median',\n",
    "        'zeros_mass_std', 'zeros_mass_cnt', 'zeros_mass_max']\n",
    "df_surge_0_first = df[df.prev_1 == 0].reset_index(drop=True)\n",
    "df_surge_0_first['target_surge_0_first'] = (df_surge_0_first['target'] > 25) * 1\n",
    "train_cols_class_0 = df_surge_0_first.columns.difference(key_cols + cat_cols + dels +\\\n",
    "                                        ['date', 'month', 'target', 'target_surge_0_first'])\n",
    "train_data = lgb.Dataset(df_surge_0_first[train_cols_class_0], label=df_surge_0_first['target_surge_0_first'])\n",
    "lgbm_class_0 = lgb.train(params=lgbm_params_class, train_set=train_data, valid_sets=[train_data],\n",
    "                       num_boost_round=20, verbose_eval=1000)\n",
    "pickle.dump(lgbm_class_0, open('./model_class_0.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
